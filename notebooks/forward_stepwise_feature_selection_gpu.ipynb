{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install scikit-learn==1.6.1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%load_ext cudf.pandas\n\nfrom cuml.svm import LinearSVR as cuLinearSVR\nfrom cuml.linear_model import Ridge as cuRidge","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import TargetEncoder, StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.metrics import make_scorer, root_mean_squared_log_error, root_mean_squared_error\nfrom tqdm import tqdm\nfrom itertools import combinations\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"users = pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")\nusers.columns = [x.lower() for x in users.columns]\n\nassert users['id'].is_unique\n# surprising!\nassert users.notnull().all().all()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# competition's objective function in mind\nusers = users.assign(calories_log1p = lambda df_: np.log(df_['calories'] + 1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"COLUMNS_TO_ONEHOT = ['sex']\nonehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='warn').set_output(transform='pandas')\nusers_onehot = onehot_encoder.fit_transform(users[COLUMNS_TO_ONEHOT])\n\nusers = pd.concat([users, users_onehot], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"COLUMNS_CATEGORICAL = [\n    'sex',\n    ]\n\nCOLUMNS_NUMERIC = [\n    'age',\n    'height',\n    'weight',\n    'duration',\n    'heart_rate',\n    'body_temp',\n    'sex_male',\n    'sex_female'\n    ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES_INTERACTED = []\n\ndegrees_interaction = [2, 3]\n\nfor d in degrees_interaction:\n    for features_interact in tqdm(list(combinations(COLUMNS_NUMERIC, d))):\n\n        interaction_title = '_'.join(features_interact)\n        FEATURES_INTERACTED.append(interaction_title)\n        \n        users[interaction_title] = users[list(features_interact)].prod(axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"variances = users.select_dtypes(include='number').var()\n\ncolumns_zero_variance = variances[variances == 0].index.tolist()\n\nusers = users.drop(columns=columns_zero_variance)\n\nFEATURES_INTERACTED = list( set(FEATURES_INTERACTED).difference(set(columns_zero_variance)) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test main effects first\n# -1.0 makes results decimals; 1 makes all zero ...\ndegrees_polynomial = [-1.0, 0.5, 2.0]\ncolumns_polynomial = []\nfor x in COLUMNS_NUMERIC:\n    for d in degrees_polynomial:\n        series = users[x]**d\n        if (all(~np.isinf(series))):\n            users[f\"{x}^{d}\"] = series\n            columns_polynomial += [f\"{x}^{d}\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# following basis expansion via interactions, test:\n    # new features set size\n    # invariant (uninformative) features?\nusers.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"XY = users","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"code","source":"def create_transformers_argument(features_model, features_universe_target_encode):\n\n    transformers = []\n\n    features_target_encode = [ftr for ftr in features_model if ftr in features_universe_target_encode]\n    if features_target_encode:\n        spec = ('target_encode', TargetEncoder(target_type='continuous',cv=3), features_target_encode)\n        transformers.append(spec)\n\n    return transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from joblib import Parallel, delayed\nimport warnings\nimport time\n\ndef fit_evaluate_marginal_model(\n    feature_marginal, \n    X_challenger, \n    y, \n    n_jobs):\n\n    features_challenger = list(X_challenger.columns)\n    transformers = create_transformers_argument(features_challenger, COLUMNS_CATEGORICAL)\n    feature_transform_pipeline = ColumnTransformer(transformers, remainder='passthrough', verbose_feature_names_out=False)\n    pipeline_e2e = Pipeline(\n        [\n            (\"transform_features\", feature_transform_pipeline), \n            (\"standard_scale\", StandardScaler()),\n\n            # expected by NVIDIA RAPIDS GPU-enabled model\n            ('to_float32', FunctionTransformer(lambda X: X.astype(np.float32))),\n            # ('to_float32', Float32Transformer()),\n            \n            # tested Support Vector Regression, but this was too time-consuming.\n            # thought about KernelRidge too, but documentation suggests, equivalent to SVR\n            # 'saga' chosen in attempt to resolve Runtime - divide by zero errors\n            # (\"model\", Ridge(alpha=1.0, solver='saga')),\n            # ('model', cuLinearSVR())\n            ('model', cuRidge())\n            \n        ]\n        )\n\n    scores = cross_val_score(\n        pipeline_e2e, \n        X_challenger, \n        y, \n        scoring='neg_root_mean_squared_error', \n        cv=5,  \n        n_jobs=n_jobs\n        )\n    score_cv_summary = scores.mean()\n\n    return (feature_marginal, -1 * score_cv_summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# it's strange to suppress a warning, but empirically and from research,\n# haven't found the call-to-action from this warning.\n# warnings.filterwarnings(\"ignore\", message=\".*A worker stopped while some jobs were given to the executor.*\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", message=\".*Changing solver to 'svd' as 'eig' or 'cd' solvers*\", category=UserWarning)\n\nstart = time.time()\n\n\nfeatures_marginal = columns_polynomial + COLUMNS_NUMERIC + FEATURES_INTERACTED\n# using features universe, model search times practically infeasible\n# features_marginal = [x for x in features_marginal if x not in features_low_importance]\nfeatures_champion = []\nfeatures_added_count = 0\n\n# dummy, to kick off procedure\nchampion_score = 10_000\nchallenger_score = 1_000\nchampions_score_sequence = [champion_score]\n\n# when challenger improves upon champion, continue extending challenger.\n# when challenger loses to champion, challenger has become too complex.\nwhile champion_score >= challenger_score:\n\n    challengers_scores = []\n    for x in features_marginal:\n\n        X = XY[features_champion + [x]]\n        y = XY['calories_log1p']\n        \n        scores_cv = []\n        kf = KFold(n_splits=5, shuffle=True, random_state=777)\n        for indexes_train, indexes_test in kf.split(XY):\n        \n            X_train, X_test = X.loc[indexes_train], X.loc[indexes_test]\n            \n            y_train, y_test = y.loc[indexes_train], y.loc[indexes_test]\n\n            features_mean = X_train.mean()\n            features_std = X_train.std()\n            X_train_trfm = (X_train - features_mean) / features_std\n            \n            model = cuRidge(alpha=0.1)\n            model.fit(X_train_trfm, y_train)\n\n            X_test_trfm = (X_test - features_mean) / features_std\n            preds = model.predict(X_test_trfm)\n\n            score = root_mean_squared_error(y_test, preds)\n            scores_cv.append(score)\n\n        score_summary = np.array(scores_cv).mean()\n        \n        challengers_scores.append( (x, score_summary) )\n\n    feature_marginal_challenger, challenger_score = min(challengers_scores, key=lambda x: x[1])\n\n    print(f\"Challenger score: {challenger_score}\")\n    print(f\"Champion score: {champion_score}\")\n    elapsed = time.time() - start\n    print(\"Cell run time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))\n\n    # if challenger improves upon champion, then replace champion with challenger.\n    if challenger_score < champion_score:\n\n        features_champion += [feature_marginal_challenger]\n        champion_score = challenger_score\n        features_marginal.remove(feature_marginal_challenger)\n        print(f\"{feature_marginal_challenger} selected in this step.\")\n\n        features_added_count += 1\n        print(f\"Model features count comes to {features_added_count}.\")\n\n    champions_score_sequence.append(champion_score)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_champion","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES_SELECTED = features_champion","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fit Pruned Model","metadata":{}},{"cell_type":"code","source":"# pipeline_e2e = Pipeline(\n#     [\n#         ('model', RandomForestRegressor(n_estimators=100))\n#      ]\n# )\n\nscores = cross_val_score(\n    RandomForestRegressor(n_estimators=100),\n    users[FEATURES_SELECTED],\n    users['calories_log1p'],\n    cv=5, \n    scoring='neg_root_mean_squared_error',\n    verbose=2\n)\n\nscores.mean(), scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}