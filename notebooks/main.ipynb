{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.metrics import make_scorer, root_mean_squared_log_error, root_mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_FEATURE_SELECTION_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\"./data/external/train.csv\")\n",
    "users.columns = [x.lower() for x in users.columns]\n",
    "\n",
    "assert users['id'].is_unique\n",
    "# surprising!\n",
    "assert users.notnull().all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition's objective function in mind\n",
    "users = users.assign(calories_log1p = lambda df_: np.log(df_['calories'] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_ONEHOT = ['sex']\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='warn').set_output(transform='pandas')\n",
    "users_onehot = onehot_encoder.fit_transform(users[COLUMNS_TO_ONEHOT])\n",
    "\n",
    "users = pd.concat([users, users_onehot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_CATEGORICAL = [\n",
    "    'sex',\n",
    "    ]\n",
    "\n",
    "COLUMNS_NUMERIC = [\n",
    "    'age',\n",
    "    'height',\n",
    "    'weight',\n",
    "    'duration',\n",
    "    'heart_rate',\n",
    "    'body_temp',\n",
    "    'sex_male',\n",
    "    'sex_female'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_polynomial = [\n",
    "#     'age',\n",
    "#     'height',\n",
    "#     'weight',\n",
    "#     'duration',\n",
    "#     'heart_rate',\n",
    "#     'body_temp',\n",
    "#     ]\n",
    "# for x in features_polynomial:\n",
    "#     users[f\"{x}_sqrt\"] = users[x]**(1/2)\n",
    "#     users[f\"{x}_sq\"] = users[x]**2\n",
    "#     COLUMNS_NUMERIC += [f\"{x}_sqrt\", f\"{x}_sq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_INTERACTED = []\n",
    "\n",
    "# adding 4th degree interactions (before polynomial transforms) failed to improve model\n",
    "degrees_interaction = [2, 3]\n",
    "\n",
    "for d in degrees_interaction:\n",
    "    for features_interact in tqdm(list(combinations(COLUMNS_NUMERIC, d))):\n",
    "\n",
    "        interaction_title = '_'.join(features_interact)\n",
    "        FEATURES_INTERACTED.append(interaction_title)\n",
    "        \n",
    "        users[interaction_title] = users[list(features_interact)].prod(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = users.select_dtypes(include='number').var()\n",
    "\n",
    "columns_zero_variance = variances[variances == 0].index.tolist()\n",
    "\n",
    "users = users.drop(columns=columns_zero_variance)\n",
    "\n",
    "FEATURES_INTERACTED = list( set(FEATURES_INTERACTED).difference(set(columns_zero_variance)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to manage features proliferation\n",
    "# mutual_information = mutual_info_regression(\n",
    "#     users[FEATURES_INTERACTED], \n",
    "#     users['calories_log1p'], \n",
    "#     n_jobs=-1\n",
    "#     )\n",
    "# mutual_information = (\n",
    "#     pd.Series(mutual_information, index=FEATURES_INTERACTED)\n",
    "#     .sort_values(ascending=False)\n",
    "#     )\n",
    "# features_low_importance = list(mutual_information[-1_000:].index)\n",
    "\n",
    "# with open(\"./models/mutual_information__features_low_importance.txt\", 'w') as file:\n",
    "#     for item in features_low_importance:\n",
    "#         file.write(f\"{item}\\n\")\n",
    "\n",
    "# with open(\"./models/mutual_information__features_low_importance.txt\", 'r') as file:\n",
    "#     features_low_importance = [line.strip() for line in file]\n",
    "\n",
    "# users = users.drop(columns=features_low_importance)\n",
    "\n",
    "# FEATURES_INTERACTED = list( set(FEATURES_INTERACTED) - set(features_low_importance) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test main effects first\n",
    "# -1.0 makes results decimals; 1 makes all zero ...\n",
    "degrees_polynomial = [-1.0, 0.5, 2.0]\n",
    "columns_polynomial = []\n",
    "for x in COLUMNS_NUMERIC:\n",
    "    for d in degrees_polynomial:\n",
    "        series = users[x]**d\n",
    "        if (all(~np.isinf(series))):\n",
    "            users[f\"{x}^{d}\"] = series\n",
    "            columns_polynomial += [f\"{x}^{d}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following basis expansion via interactions, test:\n",
    "    # new features set size\n",
    "    # invariant (uninformative) features?\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit, Evaluate Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_transform_pipeline = ColumnTransformer(\n",
    "#     [\n",
    "#         ('target_encoder', TargetEncoder(target_type='continuous',cv=3), COLUMNS_CATEGORICAL),\n",
    "#         ],\n",
    "#     remainder='passthrough',\n",
    "#     verbose_feature_names_out=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's faster: parallelize RF, and serial cv?\n",
    "# or serial RF, parallel CV?\n",
    "\n",
    "# in experiments, serial RF is faster.\n",
    "\n",
    "# serial RF, parallel CV: \n",
    "\n",
    "\n",
    "# pipeline_e2e = Pipeline(\n",
    "#     [\n",
    "#         ('feature_transform_pipeline', feature_transform_pipeline),\n",
    "#         ('model', RandomForestRegressor(n_estimators=100))\n",
    "#      ]\n",
    "# )\n",
    "\n",
    "# preds_baseline = cross_val_predict(\n",
    "#     pipeline_e2e,\n",
    "#     XY[COLUMNS_CATEGORICAL + COLUMNS_NUMERIC],\n",
    "#     XY['calories'],\n",
    "#     n_jobs=-1,\n",
    "#     cv=5,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# score_baseline = root_mean_squared_log_error(XY['calories'], preds_baseline)\n",
    "\n",
    "# score_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_e2e = Pipeline(\n",
    "#     [\n",
    "#         ('feature_transform_pipeline', feature_transform_pipeline),\n",
    "#         ('model', RandomForestRegressor(n_estimators=100))\n",
    "#      ]\n",
    "# )\n",
    "\n",
    "# scores = cross_val_score(\n",
    "#     pipeline_e2e,\n",
    "#     XY[COLUMNS_CATEGORICAL + COLUMNS_NUMERIC],\n",
    "#     XY['calories_log1p'],\n",
    "#     n_jobs=-1,\n",
    "#     cv=5,\n",
    "#     scoring='neg_root_mean_squared_error',\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# scores.mean(), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long run time:\n",
    "# summary score â‰ˆ 0.06255 (from 2024-05-05 AM run)\n",
    "\n",
    "# parallel CV, serial RF runtime > 30 min.\n",
    "# hypothesis: as features set widens, then faster to parallel RF & serial cv.\n",
    "# empirically, that works, runtime 16.5 min\n",
    "\n",
    "# pipeline_e2e = Pipeline(\n",
    "#     [\n",
    "#         ('feature_transform_pipeline', feature_transform_pipeline),\n",
    "#         ('model', RandomForestRegressor(n_estimators=100, n_jobs=-1))\n",
    "#      ]\n",
    "# )\n",
    "\n",
    "# scores = cross_val_score(\n",
    "#     pipeline_e2e,\n",
    "#     XY[COLUMNS_CATEGORICAL + COLUMNS_NUMERIC + FEATURES_INTERACTED],\n",
    "#     XY['calories_log1p'],\n",
    "#     # n_jobs=-1,\n",
    "#     cv=5,\n",
    "#     scoring='neg_root_mean_squared_error',\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# scores.mean(), scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformers_argument(features_model, features_universe_target_encode=COLUMNS_CATEGORICAL):\n",
    "\n",
    "    transformers = []\n",
    "\n",
    "    features_target_encode = [ftr for ftr in features_model if ftr in features_universe_target_encode]\n",
    "    if features_target_encode:\n",
    "        spec = ('target_encode', TargetEncoder(target_type='continuous',cv=3), features_target_encode)\n",
    "        transformers.append(spec)\n",
    "\n",
    "    return transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_model(X, y, n_jobs=1, model_title=None):\n",
    "    \"\"\"\n",
    "    `model_title` helps with parallel calls to function over `X` datasets,\n",
    "    and results need indexed.\n",
    "    \"\"\"\n",
    "\n",
    "    transformers = create_transformers_argument(list(X.columns))\n",
    "    feature_transform_pipeline = ColumnTransformer(\n",
    "        transformers,\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "    pipeline_e2e = Pipeline(\n",
    "        [\n",
    "            (\"transform_features\", feature_transform_pipeline),\n",
    "            (\"standard_scale\", StandardScaler()),\n",
    "            (\"model\", Ridge(alpha=0.1))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        pipeline_e2e,\n",
    "        X,\n",
    "        y,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    score_cv_summary = (-1 * scores.mean())\n",
    "\n",
    "    if model_title:\n",
    "        score_cv_summary = (model_title, score_cv_summary)\n",
    "\n",
    "    return score_cv_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's strange to suppress a warning, but empirically and from research,\n",
    "# haven't found the call-to-action from this warning.\n",
    "# warnings.filterwarnings(\"ignore\", message=\".*A worker stopped while some jobs were given to the executor.*\", category=UserWarning)\n",
    "\n",
    "def fit_evaluate_marginal_model(\n",
    "    feature_marginal, \n",
    "    X_challenger, \n",
    "    y, \n",
    "    n_jobs):\n",
    "\n",
    "    features_challenger = list(X_challenger.columns)\n",
    "    transformers = create_transformers_argument(features_challenger, COLUMNS_CATEGORICAL)\n",
    "    feature_transform_pipeline = ColumnTransformer(transformers, remainder='passthrough', verbose_feature_names_out=False)\n",
    "    pipeline_e2e = Pipeline(\n",
    "        [\n",
    "            (\"transform_features\", feature_transform_pipeline), \n",
    "            (\"standard_scale\", StandardScaler()),\n",
    "            # ('to_float32', Float32Transformer()),\n",
    "            # tested Support Vector Regression, but this was too time-consuming.\n",
    "            # thought about KernelRidge too, but documentation suggests, equivalent to SVR\n",
    "            # 'saga' chosen in attempt to resolve Runtime - divide by zero errors\n",
    "            # TODO: might 'sag' be better fit for large data?\n",
    "            (\"model\", Ridge(alpha=1.0, solver='saga'))\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        pipeline_e2e, \n",
    "        X_challenger, \n",
    "        y, \n",
    "        scoring='neg_root_mean_squared_error', \n",
    "        cv=5,  \n",
    "        n_jobs=n_jobs\n",
    "        )\n",
    "    score_cv_summary = scores.mean()\n",
    "\n",
    "    return (feature_marginal, -1 * score_cv_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_FEATURE_SELECTION_RUN:\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    features_marginal = COLUMNS_CATEGORICAL + COLUMNS_NUMERIC + FEATURES_INTERACTED\n",
    "    # using features universe, model search times practically infeasible\n",
    "    # features_marginal = [x for x in features_marginal if x not in features_low_importance]\n",
    "    features_champion = []\n",
    "    features_added_count = 0\n",
    "\n",
    "    # dummy, to kick off procedure\n",
    "    champion_score = 10_000\n",
    "    challenger_score = 1_000\n",
    "    champions_score_sequence = [champion_score]\n",
    "\n",
    "    # when challenger improves upon champion, continue extending challenger.\n",
    "    # when challenger loses to champion, challenger has become too complex.\n",
    "    while champion_score >= challenger_score:\n",
    "\n",
    "        challengers_scores = Parallel(n_jobs=12)(\n",
    "            delayed(fit_evaluate_marginal_model)(x, XY[features_champion + [x]], XY['calories_log1p'], 1) \n",
    "            for x in features_marginal\n",
    "        )\n",
    "\n",
    "        feature_marginal_challenger, challenger_score = min(challengers_scores, key=lambda x: x[1])\n",
    "\n",
    "        print(f\"Challenger score: {challenger_score}\")\n",
    "        print(f\"Champion score: {champion_score}\")\n",
    "        elapsed = time.time() - start\n",
    "        print(\"Cell run time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))\n",
    "\n",
    "        # if challenger improves upon champion, then replace champion with challenger.\n",
    "        if challenger_score < champion_score:\n",
    "\n",
    "            features_champion += [feature_marginal_challenger]\n",
    "            champion_score = challenger_score\n",
    "            features_marginal.remove(feature_marginal_challenger)\n",
    "            print(f\"{feature_marginal_challenger} selected in this step.\")\n",
    "\n",
    "            features_added_count += 1\n",
    "            print(f\"Model features count comes to {features_added_count}.\")\n",
    "\n",
    "        champions_score_sequence.append(champion_score)\n",
    "\n",
    "    features_champion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from GPU-powered runs\n",
    "\n",
    "FEATURES_SELECTED_RIDGE_DEFAULT_PENALTY = [\n",
    "    'body_temp',\n",
    "    'heart_rate',\n",
    "    'age_duration',\n",
    "    'age_duration_body_temp',\n",
    "    'duration',\n",
    "    'duration_heart_rate_body_temp',\n",
    "    'duration_body_temp',\n",
    "    'age_body_temp',\n",
    "    'sex_male',\n",
    "    'age_heart_rate_sex_female',\n",
    "    'height_heart_rate_sex_male',\n",
    "    'weight_body_temp_sex_female',\n",
    "    'age_heart_rate_body_temp',\n",
    "    'age_body_temp_sex_female',\n",
    "    'heart_rate_sex_female',\n",
    "    'height_weight_sex_male'\n",
    "]\n",
    "\n",
    "FEATURES_SELECTED_RIDGE_ALPHA_0P1 = [\n",
    "    'body_temp',\n",
    "    'heart_rate',\n",
    "    'age_duration',\n",
    "    'age_duration_body_temp',\n",
    "    'duration',\n",
    "    'duration_heart_rate_body_temp',\n",
    "    'duration_body_temp',\n",
    "    'duration_heart_rate',\n",
    "    'age',\n",
    "    'sex_female',\n",
    "    'age_heart_rate_sex_female',\n",
    "    'height_heart_rate_sex_male',\n",
    "    'weight_body_temp_sex_female',\n",
    "    'age_heart_rate',\n",
    "    'age_body_temp_sex_male',\n",
    "    'heart_rate_sex_male',\n",
    "    'height_weight_sex_male',\n",
    "    'duration_heart_rate_sex_male',\n",
    "    'duration_heart_rate_sex_female',\n",
    "    'duration_sex_male',\n",
    "    'weight_sex_female',\n",
    "    'duration_sex_female',\n",
    "    'age_duration_heart_rate',\n",
    "    'age_heart_rate_body_temp',\n",
    "    'age_body_temp',\n",
    "    'height_body_temp_sex_male',\n",
    "    'age_weight_sex_female',\n",
    "    'duration_body_temp_sex_female',\n",
    "    'duration_body_temp_sex_male',\n",
    "    'age_weight_heart_rate',\n",
    "    'height_sex_female',\n",
    "    'age_height_heart_rate',\n",
    "    'height_duration_sex_male',\n",
    "    'heart_rate_body_temp_sex_female',\n",
    "    'weight_heart_rate_sex_female',\n",
    "    'age_weight_body_temp',\n",
    "    'weight_body_temp_sex_male',\n",
    "    'age_height_sex_female',\n",
    "    'age_weight',\n",
    "    'weight_duration_body_temp',\n",
    "    'weight_duration',\n",
    "    'weight_duration_heart_rate',\n",
    "    'age_sex_male',\n",
    "    'weight_heart_rate_sex_male',\n",
    "    'weight_heart_rate_body_temp',\n",
    "    'age_height_body_temp',\n",
    "    'heart_rate_body_temp_sex_male',\n",
    "    'heart_rate_sex_female',\n",
    "    'weight_heart_rate',\n",
    "    'height_heart_rate_sex_female',\n",
    "    'weight',\n",
    "    'height_body_temp',\n",
    "    'age_duration_sex_male',\n",
    "    'heart_rate_body_temp',\n",
    "    'weight_body_temp',\n",
    "    'height_weight',\n",
    "    'age_height_weight',\n",
    "    'height_weight_heart_rate',\n",
    "    'height_heart_rate_body_temp',\n",
    "    'sex_male',\n",
    "    'height_body_temp_sex_female',\n",
    "    'weight_sex_male',\n",
    "    'height_duration_heart_rate',\n",
    "    'height_duration_sex_female',\n",
    "    'height_duration_body_temp',\n",
    "    'height_duration',\n",
    "    'age_body_temp_sex_female',\n",
    "    'weight_duration_sex_female'\n",
    "    ]\n",
    "\n",
    "FEATURES_SELECTED_POLYNOMIALS_RIDGE_ALPHA_0P1 = [\n",
    "    'duration^0.5',\n",
    "    'duration',\n",
    "    'heart_rate^-1.0',\n",
    "    'age^0.5',\n",
    "    'duration^2.0',\n",
    "    'sex_female^0.5',\n",
    "    'age_heart_rate_sex_female',\n",
    "    'height_heart_rate_sex_male',\n",
    "    'age_weight_duration',\n",
    "    'age_body_temp_sex_male',\n",
    "    'age_height_heart_rate',\n",
    "    'age_height_body_temp',\n",
    "    'duration^-1.0',\n",
    "    'heart_rate_sex_female',\n",
    "    'age_weight_sex_male',\n",
    "    'age_heart_rate_sex_male',\n",
    "    'duration_heart_rate_sex_female',\n",
    "    'duration_body_temp_sex_female',\n",
    "    'body_temp_sex_female',\n",
    "    'weight_sex_female',\n",
    "    'weight_heart_rate',\n",
    "    # 'weight_duration_sex_male',\n",
    "    # 'age_sex_male',\n",
    "    # 'age_height_weight',\n",
    "    # 'duration_sex_male',\n",
    "    # 'weight_heart_rate_sex_female',\n",
    "    # 'age_body_temp_sex_female',\n",
    "    # 'heart_rate_body_temp_sex_female',\n",
    "    # 'weight_duration_heart_rate',\n",
    "    # 'heart_rate^0.5'\n",
    "    ]\n",
    "\n",
    "FEATURES_SELECTED_SVR_C_0P1 = [\n",
    "    'body_temp',\n",
    "    'heart_rate',\n",
    "    'age_duration',\n",
    "    'age_duration_body_temp',\n",
    "    'duration',\n",
    "    'duration_heart_rate_body_temp',\n",
    "    'age_weight_duration',\n",
    "    'weight',\n",
    "    'age_height_weight',\n",
    "    'duration_body_temp',\n",
    "    'weight_heart_rate_body_temp',\n",
    "    'age_height_heart_rate',\n",
    "    'sex_female',\n",
    "    'age_duration_sex_female',\n",
    "    'weight_heart_rate_sex_female',\n",
    "    'age_sex_male',\n",
    "    'age_weight_sex_female',\n",
    "    'age_height',\n",
    "    'heart_rate_body_temp_sex_male',\n",
    "    'weight_body_temp'\n",
    "    ]\n",
    "\n",
    "FEATURES_SELECTED_SVR = [\n",
    "    'body_temp',\n",
    "    'heart_rate',\n",
    "    'age_duration',\n",
    "    'age_duration_body_temp',\n",
    "    'duration',\n",
    "    'duration_heart_rate_body_temp',\n",
    "    'age_weight_duration',\n",
    "    'weight_body_temp',\n",
    "    'age_height_weight',\n",
    "    'duration_body_temp',\n",
    "    'weight_heart_rate',\n",
    "    'sex_female',\n",
    "    'height_heart_rate_sex_female',\n",
    "    'age_sex_male',\n",
    "    'weight',\n",
    "    'height_duration_sex_female'\n",
    "]\n",
    "\n",
    "FEATURES_SELECTED_SVR_DEGREE4 = [\n",
    "    'body_temp',\n",
    "    'heart_rate',\n",
    "    'age_duration',\n",
    "    'age_duration_body_temp',\n",
    "    'duration',\n",
    "    'duration_heart_rate_body_temp',\n",
    "    'age_weight_duration_heart_rate',\n",
    "    'sex_male',\n",
    "    'age_heart_rate_body_temp_sex_male',\n",
    "    'height_heart_rate_body_temp_sex_male',\n",
    "    'duration_body_temp',\n",
    "    'age_weight',\n",
    "    'age_height_weight_heart_rate',\n",
    "    'weight_heart_rate_sex_female',\n",
    "    'age_sex_male',\n",
    "    'heart_rate_body_temp_sex_female'\n",
    "]\n",
    "\n",
    "# FEATURES_SELECTED_SVR_DEGREES_2_3_INTERACTIONS = [\n",
    "#     'duration_sqrt',\n",
    "#     'duration',\n",
    "#     'age_sqrt_duration_sqrt_heart_rate_sq',\n",
    "#     'age_duration_heart_rate_sq',\n",
    "#     'age_sq_duration_sq_body_temp_sqrt',\n",
    "#     'height_sqrt_duration_sqrt_duration_sq',\n",
    "#     'sex_male',\n",
    "#     'age_weight_sqrt_duration_sq',\n",
    "#     'duration_age_sq_heart_rate_sq',\n",
    "#     'age_heart_rate_duration_sq',\n",
    "#     'age_age_sq_duration_sq',\n",
    "#     'body_temp_heart_rate_sqrt',\n",
    "#     'heart_rate_body_temp_sq',\n",
    "#     'heart_rate_duration_sq_body_temp_sq',\n",
    "#     'duration_age_sqrt_duration_sq',\n",
    "#     'age',\n",
    "#     'body_temp_heart_rate_sqrt_body_temp_sq',\n",
    "#     'duration_sqrt_duration_sq_body_temp_sqrt',\n",
    "#     'heart_rate_height_sq_duration_sq',\n",
    "#     'age_sq_duration_sqrt_duration_sq'\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES_SELECTED = list( \n",
    "#     set(FEATURES_SELECTED_RIDGE) \n",
    "#     | set(FEATURES_SELECTED_SVR) \n",
    "#     | set(FEATURES_SELECTED_SVR_DEGREE4)\n",
    "#     )\n",
    "\n",
    "# FEATURES_SELECTED = FEATURES_SELECTED_POLYNOMIALS_RIDGE_ALPHA_0P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Contributors (IC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "\n",
    "#     params = {\n",
    "#         \"n_estimators\": 1_000,\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": \"gbtree\",\n",
    "#         # approximately sorted on hyperparam influence, descending\n",
    "#         \"eta\": trial.suggest_float(\"eta\", 1e-2, 0.1, log=True),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 1, 21, step=2),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "#         }\n",
    "\n",
    "#     scores = cross_val_score(\n",
    "#         xgb.XGBRegressor(**params), \n",
    "\n",
    "#         XY[FEATURES_SELECTED],\n",
    "\n",
    "#         XY['calories_log1p'], \n",
    "\n",
    "#         # as optuna _minimizes_, beware initially negative result\n",
    "#         scoring='neg_root_mean_squared_error', \n",
    "\n",
    "#         cv=5\n",
    "#         )\n",
    "#     score_cv_summary = scores.mean()\n",
    "\n",
    "#     return -1 * score_cv_summary\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     # as hyperparameters set size increases, likely need more trials\n",
    "#     n_trials=35,\n",
    "#     # if optuna returns nulls in y_pred, don't fail the entire study\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=1 * 60 * 60,\n",
    "# )\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect ~0.6 for any individual model\n",
    "\n",
    "CHAMPION_PARAMS_SVR_0P1_FEATURES = {\n",
    "    \"n_estimators\": 1_000,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    'eta': 0.017685285832108887,\n",
    "    'max_depth': 7,\n",
    "    'subsample': 0.9081335480436096,\n",
    "    'colsample_bytree': 0.7181715686597967,\n",
    "    'lambda': 0.0031679606274101833,\n",
    "    'alpha': 1.3956904209433707e-07,\n",
    "    'gamma': 7.710204038839976e-05\n",
    " }\n",
    "\n",
    "CHAMPION_PARAMS_HYBRID_FEATURES = {\n",
    "    \"n_estimators\": 1_000,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    'eta': 0.021941020992632378,\n",
    "    'max_depth': 7,\n",
    "    'subsample': 0.8180872703144669,\n",
    "    'colsample_bytree': 0.642916966044508,\n",
    "    'lambda': 3.24525375145767e-07,\n",
    "    'alpha': 0.9854875114734817,\n",
    "    'gamma': 3.62461567739456e-05\n",
    " }\n",
    "\n",
    "CHAMPION_PARAMS_RIDGE_0P1_FEATURES = {\n",
    "    \"n_estimators\": 1_000,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    'eta': 0.02497863296780753,\n",
    "    'max_depth': 15,\n",
    "    'subsample': 0.9927172119082496,\n",
    "    'colsample_bytree': 0.22427335166711926,\n",
    "    'lambda': 9.812890689973725e-06,\n",
    "    'alpha': 0.4386722082854982,\n",
    "    'gamma': 0.01035902889930934\n",
    "    }\n",
    "\n",
    "CHAMPION_PARAMS_POLYNOMIALS_RIDGE_0P1 = {\n",
    "    \"n_estimators\": 1_000,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    'eta': 0.012344853649482653,\n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.6835261426364755,\n",
    "    'colsample_bytree': 0.898511409057206,\n",
    "    'lambda': 0.0008844118731907166,\n",
    "    'alpha': 5.021206527308231e-08,\n",
    "    'gamma': 4.558181241103821e-07\n",
    "    }\n",
    "\n",
    "# expect ~0.06 score\n",
    "CHAMPION_PARAMS_RIDGE_V1_FEATURES = {\n",
    "    'eta': 0.009447724417129216,\n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.7975742692448574,\n",
    "    'colsample_bytree': 0.46241012463825576,\n",
    "    'lambda': 1.0805178355851031e-06,\n",
    "    'alpha': 0.9133372453796443,\n",
    "    'gamma': 0.00396569352508906\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = cross_val_score(\n",
    "#     xgb.XGBRegressor(**CHAMPION_PARAMS_POLYNOMIALS_RIDGE_0P1), \n",
    "#     XY[FEATURES_SELECTED_SVR_C_0P1],\n",
    "#     XY['calories_log1p'], \n",
    "#     # as optuna _minimizes_, beware initially negative result\n",
    "#     scoring='neg_root_mean_squared_error', \n",
    "#     cv=5\n",
    "#     )\n",
    "\n",
    "# scores.mean(), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "\n",
    "#     params = {\n",
    "#         \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 5, 55, step=10),\n",
    "#         \"weights\": trial.suggest_categorical(\"weights\", ['uniform', 'distance'])\n",
    "#         }\n",
    "\n",
    "#     scores = cross_val_score(\n",
    "#         Pipeline([ ('standardize', StandardScaler()), ('model', KNeighborsRegressor(**params)) ]), \n",
    "#         XY[FEATURES_SELECTED_RIDGE_DEFAULT_PENALTY],\n",
    "#         XY['calories_log1p'], \n",
    "#         # as optuna _minimizes_, beware initially negative result\n",
    "#         scoring='neg_root_mean_squared_error', \n",
    "#         cv=5\n",
    "#         )\n",
    "#     score_cv_summary = scores.mean()\n",
    "\n",
    "#     return -1 * score_cv_summary\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=20,\n",
    "#     # if optuna returns nulls in y_pred, don't fail the entire study\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=1 * 60 * 60,\n",
    "# )\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEnsemble(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Aggregate several lower-level models.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "\n",
    "    def fit_components(self, X, y):\n",
    "        \"\"\"\n",
    "        Ensemble model learns from component models' out-of-fold predictions,\n",
    "        otherwise expect overfitting.\n",
    "        Persist models' predictions to allow a separate, dedicated step \n",
    "        to fit-evaluate ensemble blender.\n",
    "\n",
    "        Model experimentation notes:\n",
    "        - Expect radial basis function kernel fit time is prohibitive with data of ~750K obs.\n",
    "        - Insignificant benefit from adding KNN \n",
    "        - Insignficant benefit from adding random forest\n",
    "        \"\"\"\n",
    "\n",
    "        XY_ensemble = y.to_frame('y')\n",
    "\n",
    "        preds_svr_default__xgb = cross_val_predict(\n",
    "            xgb.XGBRegressor(**CHAMPION_PARAMS_HYBRID_FEATURES), \n",
    "            X[FEATURES_SELECTED_SVR],\n",
    "            y, \n",
    "            cv=5,\n",
    "            verbose=1\n",
    "            )\n",
    "        svr_default__xgb = xgb.XGBRegressor(**CHAMPION_PARAMS_HYBRID_FEATURES)\n",
    "        svr_default__xgb.fit(X[FEATURES_SELECTED_SVR], y)\n",
    "        self.models['svr_default__xgb'] = svr_default__xgb\n",
    "        XY_ensemble = XY_ensemble.assign(svr_default__xgb = preds_svr_default__xgb)\n",
    "        print(\"SVR (default penalty) feature selection - xgboost fit.\")\n",
    "        \n",
    "        preds_svr_0p1 = cross_val_predict(\n",
    "            xgb.XGBRegressor(**CHAMPION_PARAMS_HYBRID_FEATURES), \n",
    "            X[FEATURES_SELECTED_SVR_C_0P1],\n",
    "            y, \n",
    "            cv=5,\n",
    "            verbose=1\n",
    "            )\n",
    "        svr_0p1__xgb = xgb.XGBRegressor(**CHAMPION_PARAMS_HYBRID_FEATURES)\n",
    "        svr_0p1__xgb.fit(X[FEATURES_SELECTED_SVR_C_0P1], y)\n",
    "        self.models['svr_0p1__xgb'] = svr_0p1__xgb\n",
    "        XY_ensemble = XY_ensemble.assign(svr_0p1__xgb = preds_svr_0p1)\n",
    "        print(\"SVR (below-default penalty) feature selection - xgboost fit.\")\n",
    "\n",
    "        preds_ridge_default__xgb = cross_val_predict(\n",
    "            xgb.XGBRegressor(**CHAMPION_PARAMS_RIDGE_V1_FEATURES), \n",
    "            X[FEATURES_SELECTED_RIDGE_DEFAULT_PENALTY],\n",
    "            y, \n",
    "            cv=5,\n",
    "            verbose=1\n",
    "            )\n",
    "        ridge_default__xgb = xgb.XGBRegressor(**CHAMPION_PARAMS_RIDGE_V1_FEATURES)\n",
    "        ridge_default__xgb.fit(X[FEATURES_SELECTED_RIDGE_DEFAULT_PENALTY], y)\n",
    "        self.models['ridge_default__xgb'] = ridge_default__xgb\n",
    "        XY_ensemble = XY_ensemble.assign(ridge_default__xgb = preds_ridge_default__xgb)\n",
    "        print(\"Ridge (default penalty) feature selection - xgboost fit.\")\n",
    "\n",
    "        preds_ridge_0p1 = cross_val_predict(\n",
    "            xgb.XGBRegressor(**CHAMPION_PARAMS_RIDGE_0P1_FEATURES), \n",
    "            X[FEATURES_SELECTED_RIDGE_ALPHA_0P1],\n",
    "            y, \n",
    "            cv=5,\n",
    "            verbose=1\n",
    "            )\n",
    "        ridge_0p1__xgb = xgb.XGBRegressor(**CHAMPION_PARAMS_RIDGE_0P1_FEATURES)\n",
    "        ridge_0p1__xgb.fit(X[FEATURES_SELECTED_RIDGE_ALPHA_0P1], y)\n",
    "        self.models['ridge_0p1__xgb'] = ridge_0p1__xgb\n",
    "        XY_ensemble = XY_ensemble.assign(ridge_0p1__xgb = preds_ridge_0p1)\n",
    "        print(\"Ridge (below-default penalty) feature selection - xgboost fit.\")\n",
    "\n",
    "        preds_polynomials_ridge_0p1 = cross_val_predict(\n",
    "            xgb.XGBRegressor(**CHAMPION_PARAMS_POLYNOMIALS_RIDGE_0P1), \n",
    "            X[FEATURES_SELECTED_POLYNOMIALS_RIDGE_ALPHA_0P1],\n",
    "            y, \n",
    "            cv=5,\n",
    "            verbose=1\n",
    "            )\n",
    "        ridge_polynomials_0p1__xgb = xgb.XGBRegressor(**CHAMPION_PARAMS_POLYNOMIALS_RIDGE_0P1)\n",
    "        ridge_polynomials_0p1__xgb.fit(X[FEATURES_SELECTED_POLYNOMIALS_RIDGE_ALPHA_0P1], y)\n",
    "        self.models['ridge_polynomials_0p1__xgb'] = ridge_polynomials_0p1__xgb\n",
    "        XY_ensemble = XY_ensemble.assign(ridge_polynomials_0p1__xgb = preds_polynomials_ridge_0p1)\n",
    "        print(\"Ridge (below-default penalty) polynomials feature selection - xgboost fit.\")\n",
    "\n",
    "        preds_ridge_0p1__ridge = cross_val_predict(\n",
    "            Pipeline([ ('standardize', StandardScaler()), ('model', Ridge(alpha=0.1)) ]), \n",
    "            X[FEATURES_SELECTED_RIDGE_ALPHA_0P1],\n",
    "            y, \n",
    "            cv=5,\n",
    "            verbose=1\n",
    "            )\n",
    "        ridge_0p1__ridge = Ridge(alpha=0.1)\n",
    "        ridge_0p1__ridge.fit(X[FEATURES_SELECTED_RIDGE_ALPHA_0P1], y)\n",
    "        self.models['ridge_0p1__ridge'] = ridge_0p1__ridge\n",
    "        XY_ensemble = XY_ensemble.assign(ridge_0p1__ridge = preds_ridge_0p1__ridge)\n",
    "        print(\"Ridge (below-default penalty) feature selection - Ridge fit.\")\n",
    "\n",
    "        # tested, but public LB performance declined\n",
    "        # preds_ridge_polynomials_0p1__ridge = cross_val_predict(\n",
    "        #     Pipeline([ ('standardize', StandardScaler()), ('model', Ridge(alpha=0.1)) ]), \n",
    "        #     X[FEATURES_SELECTED_POLYNOMIALS_RIDGE_ALPHA_0P1],\n",
    "        #     y, \n",
    "        #     cv=5,\n",
    "        #     verbose=1\n",
    "        #     )\n",
    "        # ridge_polynomials_0p1__ridge = Ridge(alpha=0.1)\n",
    "        # ridge_polynomials_0p1__ridge.fit(X[FEATURES_SELECTED_POLYNOMIALS_RIDGE_ALPHA_0P1], y)\n",
    "        # self.models['ridge_polynomials_0p1__ridge'] = ridge_polynomials_0p1__ridge\n",
    "        # XY_ensemble = XY_ensemble.assign(ridge_polynomials_0p1__ridge = preds_ridge_polynomials_0p1__ridge)\n",
    "        # print(\"Ridge (below-default penalty) polynomials feature selection - Ridge fit.\")\n",
    "        \n",
    "        self.XY_ensemble_fit = XY_ensemble\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.fit_components(X, y)\n",
    "\n",
    "        blender_pipeline_e2e = Pipeline(\n",
    "            [\n",
    "                (\"standard_scale\", StandardScaler()),\n",
    "                # taken from CV step\n",
    "                (\"model\", Ridge(alpha=30))\n",
    "            ]\n",
    "            )\n",
    "        blender_pipeline_e2e.fit(\n",
    "            self.XY_ensemble_fit.drop(columns='y'),\n",
    "            self.XY_ensemble_fit['y']\n",
    "            )\n",
    "        self.blender = blender_pipeline_e2e\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        X_models = []\n",
    "        for title, model in self.models.items():\n",
    "            predictions = model.predict(X[model.feature_names_in_])\n",
    "            predictions = pd.Series(predictions, name=title)\n",
    "            X_models.append(predictions)\n",
    "\n",
    "        X_models = pd.concat(X_models, axis=1)\n",
    "\n",
    "        preds = self.blender.predict(X_models)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ensemble = ModelEnsemble()\n",
    "model_ensemble.fit_components(XY.drop(columns='calories_log1p'), XY['calories_log1p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "features_marginal = list(model_ensemble.XY_ensemble_fit.drop(columns='y'))\n",
    "# using features universe, model search times practically infeasible\n",
    "# features_marginal = [x for x in features_marginal if x not in features_low_importance]\n",
    "features_champion = []\n",
    "features_added_count = 0\n",
    "\n",
    "# dummy, to kick off procedure\n",
    "champion_score = 10_000\n",
    "challenger_score = 1_000\n",
    "champions_score_sequence = [champion_score]\n",
    "\n",
    "# when challenger improves upon champion, continue extending challenger.\n",
    "# when challenger loses to champion, challenger has become too complex.\n",
    "while champion_score >= challenger_score:\n",
    "\n",
    "    challengers_scores = Parallel(n_jobs=12)(\n",
    "        delayed(fit_evaluate_model)(\n",
    "            model_ensemble.XY_ensemble_fit[features_champion + [x]], \n",
    "            model_ensemble.XY_ensemble_fit['y'], \n",
    "            model_title=x\n",
    "            )\n",
    "        for x in features_marginal\n",
    "    )\n",
    "\n",
    "    feature_marginal_challenger, challenger_score = min(challengers_scores, key=lambda x: x[1])\n",
    "\n",
    "    print(f\"Challenger score: {challenger_score}\")\n",
    "    print(f\"Champion score: {champion_score}\")\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Cell run time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))\n",
    "\n",
    "    # if challenger improves upon champion, then replace champion with challenger.\n",
    "    if challenger_score < champion_score:\n",
    "\n",
    "        features_champion += [feature_marginal_challenger]\n",
    "        champion_score = challenger_score\n",
    "        features_marginal.remove(feature_marginal_challenger)\n",
    "        print(f\"{feature_marginal_challenger} selected in this step.\")\n",
    "\n",
    "        features_added_count += 1\n",
    "        print(f\"Model features count comes to {features_added_count}.\")\n",
    "\n",
    "    champions_score_sequence.append(champion_score)\n",
    "\n",
    "features_champion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_ensemble = model_ensemble.XY_ensemble_fit.copy()\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    parameters = {'alpha': trial.suggest_float(\"alpha\", 1e-1, 1_000, log=True)}\n",
    "\n",
    "    pipeline_e2e = Pipeline(\n",
    "        [\n",
    "            (\"standard_scale\", StandardScaler()),\n",
    "            (\"model\", Ridge(**parameters))\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        pipeline_e2e, \n",
    "        XY_ensemble.drop(columns='y'),\n",
    "        XY_ensemble['y'], \n",
    "        # as optuna _minimizes_, beware initially negative result\n",
    "        scoring='neg_root_mean_squared_error', \n",
    "        cv=5\n",
    "        )\n",
    "    score_cv_summary = scores.mean()\n",
    "\n",
    "    return -1 * score_cv_summary\n",
    "\n",
    "study = optuna.create_study()\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    # as hyperparameters set size increases, likely need more trials\n",
    "    n_trials=35,\n",
    "    # if optuna returns nulls in y_pred, don't fail the entire study\n",
    "    catch=(ValueError,),\n",
    "    n_jobs=-1,\n",
    "    timeout=1 * 60 * 60,\n",
    ")\n",
    "\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does SVR ensembler deliver better results?\n",
    "# rbf kernel permits interactions. but with 750K observations, seems infeasible.\n",
    "# LinearSVR should scale better to large data, versus SVR with kernel='linear'.\n",
    "# in experiments, SVR ensembler performs ~.001 worse versus Ridge.\n",
    "\n",
    "# XY_ensemble = model_ensemble.XY_ensemble_fit.copy()\n",
    "\n",
    "# def objective(trial):\n",
    "    \n",
    "#     parameters = {\n",
    "#         'C': trial.suggest_float(\"alpha\", 1e-2, 100, log=True)\n",
    "#         }\n",
    "\n",
    "#     pipeline_e2e = Pipeline(\n",
    "#         [\n",
    "#             (\"standard_scale\", StandardScaler()),\n",
    "#             (\"model\", LinearSVR(**parameters))\n",
    "#         ]\n",
    "#         )\n",
    "\n",
    "#     scores = cross_val_score(\n",
    "#         pipeline_e2e, \n",
    "#         XY_ensemble.drop(columns='y'),\n",
    "#         XY_ensemble['y'], \n",
    "#         # as optuna _minimizes_, beware initially negative result\n",
    "#         scoring='neg_root_mean_squared_error', \n",
    "#         cv=5\n",
    "#         )\n",
    "#     score_cv_summary = scores.mean()\n",
    "\n",
    "#     return -1 * score_cv_summary\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     # as hyperparameters set size increases, likely need more trials\n",
    "#     n_trials=10,\n",
    "#     # if optuna returns nulls in y_pred, don't fail the entire study\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=1 * 60 * 60,\n",
    "# )\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ensemble's ~4 component models,\n",
    "# gradient boosting machine is a lower-performing blender (~0.6 score).\n",
    "# Implies hypothesis -> prediction components are relatively strong signals.\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     params = {\n",
    "#         \"n_estimators\": 1_000,\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": \"gbtree\",\n",
    "#         # approximately sorted on hyperparam influence, descending\n",
    "#         \"eta\": trial.suggest_float(\"eta\", 1e-2, 0.1, log=True),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 1, 21, step=2),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "#         }\n",
    "\n",
    "#     scores = cross_val_score(\n",
    "#         xgb.XGBRegressor(**params), \n",
    "#         XY_ensemble.drop(columns='y'),\n",
    "#         XY_ensemble['y'], \n",
    "#         scoring='neg_root_mean_squared_error', \n",
    "#         cv=5\n",
    "#         )\n",
    "#     score_cv_summary = scores.mean()\n",
    "\n",
    "#     return -1 * score_cv_summary\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     # as hyperparameters set size increases, likely need more trials\n",
    "#     n_trials=35,\n",
    "#     # if optuna returns nulls in y_pred, don't fail the entire study\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=1 * 60 * 60,\n",
    "# )\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting, Random Forest also a poor blender ... too complex?\n",
    "# scores = cross_val_score(\n",
    "#     RandomForestRegressor(), \n",
    "#     XY_ensemble.drop(columns='y'),\n",
    "#     XY_ensemble['y'], \n",
    "#     scoring='neg_root_mean_squared_error', \n",
    "#     cv=5,\n",
    "#     n_jobs=-1\n",
    "#     )\n",
    "# score_cv_summary = scores.mean()\n",
    "\n",
    "# score_cv_summary, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ENSEMBLE = ModelEnsemble()\n",
    "MODEL_ENSEMBLE.fit(XY.drop(columns='calories_log1p'), XY['calories_log1p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: Kuhn's Neural Net model performance, with noise features screened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import autokeras as ak\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Input, Activation\n",
    "from keras import Model\n",
    "\n",
    "def build_model(size):\n",
    "\n",
    "    x_in = Input(shape=(size,))\n",
    "    x = Dense(512)(x_in)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = XY_ensemble.drop(columns='y')\n",
    "y = XY_ensemble['y']\n",
    "\n",
    "scores_cv = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for indexes_train, indexes_test in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X.loc[indexes_train], X.loc[indexes_test]\n",
    "    \n",
    "    y_train, y_test = y.loc[indexes_train], y.loc[indexes_test]\n",
    "\n",
    "    features_mean = X_train.mean()\n",
    "    features_std = X_train.std()\n",
    "    X_train_trfm = (X_train - features_mean) / features_std\n",
    "\n",
    "    # model = Sequential([\n",
    "    #     Dense(128, activation='relu', input_shape=(X_train_trfm.shape[1],)),\n",
    "    #     Dense(64, activation='relu'),\n",
    "    #     Dense(1)  # Output layer for regression\n",
    "    # ])\n",
    "    # https://www.kaggle.com/code/garrickchinnis/calorie-expenditure-keras\n",
    "    # TODO: how does this notebook achieve a good score?\n",
    "    model = keras.Sequential([\n",
    "        Dense(128, activation='relu', input_shape=[X_train_trfm.shape[1]]),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)])\n",
    "    model = build_model(X_train_trfm.shape[1])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    # model = ak.TabularRegressor(max_trials=10, loss='mse')\n",
    "    # model.fit(X_train_trfm, y_train, epochs=50)\n",
    "\n",
    "    history = model.fit(X_train_trfm, y_train, epochs=4, batch_size=512)\n",
    "    \n",
    "    X_test_trfm = (X_test - features_mean) / features_std\n",
    "    # test_loss = np.sqrt(model.evaluate(X_test_trfm, y_test))\n",
    "    preds = model.predict(X_test_trfm)\n",
    "    test_loss = root_mean_squared_error(y_test, preds)\n",
    "    print(f\"CV RMSE: {test_loss}\")\n",
    "\n",
    "    scores_cv.append(test_loss)\n",
    "\n",
    "# keras `history` abstraction presents training set loss.\n",
    "# expect far-optimistic evaluation metric ...\n",
    "\n",
    "# TODO: cross_val_score equivalent for keras workflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.plot.scatter('weight_duration_heart_rate', 'calories');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.plot.scatter('duration', 'calories_log1p');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.plot.scatter('age_duration', 'calories');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.plot.scatter('duration_sex_female', 'calories');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.plot.scatter('age', 'calories');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.plot.scatter('height', 'calories');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.plot.scatter('weight', 'calories');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the competition's objective function differs from MSE -- RF trains on MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_test = pd.read_csv(\"./data/external/test.csv\")\n",
    "users_test.columns = [x.lower() for x in users_test.columns]\n",
    "\n",
    "users_test_onehot = onehot_encoder.transform(users_test[COLUMNS_TO_ONEHOT])\n",
    "\n",
    "users_test = pd.concat([users_test, users_test_onehot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in degrees_interaction:\n",
    "    for features_interact in tqdm(list(combinations(COLUMNS_NUMERIC, d))):\n",
    "\n",
    "        interaction_title = '_'.join(features_interact)\n",
    "        FEATURES_INTERACTED.append(interaction_title)\n",
    "        \n",
    "        users_test[interaction_title] = users_test[list(features_interact)].prod(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test main effects first\n",
    "# -1.0 makes results decimals; 1 makes all zero ...\n",
    "degrees_polynomial = [-1.0, 0.5, 2.0]\n",
    "columns_polynomial = []\n",
    "for x in COLUMNS_NUMERIC:\n",
    "    for d in degrees_polynomial:\n",
    "        series = users_test[x]**d\n",
    "        if (all(~np.isinf(series))):\n",
    "            users_test[f\"{x}^{d}\"] = series\n",
    "            columns_polynomial += [f\"{x}^{d}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_log1p_test = MODEL_ENSEMBLE.predict(users_test)\n",
    "yhat_test = np.exp(yhat_log1p_test) - 1\n",
    "\n",
    "users_test_submit = users_test.assign(Calories = yhat_test)[['id', 'Calories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_test_submit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['calories'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_test_submit.to_csv(\"./data/processed/users_test_ensemble_v3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_calories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
